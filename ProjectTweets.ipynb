{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac2e33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11967/3501930405.py:13: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/tmp/ipykernel_11967/3501930405.py:14: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "# Importing PySpark related libraries\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import concat_ws, regexp_replace, col, lower, to_date, date_format\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "# Importing MongoDB related library\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Importing Dash and Plotly for data visualization\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Additional PySpark functions and features\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, col\n",
    "\n",
    "# Importing NLTK for natural language processing\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Downloading NLTK datasets if needed\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# Importing datetime library\n",
    "from datetime import timedelta, datetime, date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf717f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ProjectTweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ffa58",
   "metadata": {},
   "source": [
    "# DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91573341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('/user1/ProjectTweets.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61bd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5363c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c358289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['ids', 'date', 'flag', 'user', 'text']\n",
    "\n",
    "for i, column_name in enumerate(new_cols):\n",
    "    df = df.withColumnRenamed(df.columns[i + 1], column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb99dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.legacy.timeParserPolicy', 'LEGACY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f77ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_column = df.select('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2baf2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date', to_date(df['date'], 'EEE MMM dd HH:mm:ss zzz yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a856b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date', to_date(col('date'), 'dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06d9721a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28086db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "572dff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to lowercase and clean unnecessary characters\n",
    "df = df.withColumn(\"text\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc3289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special symbols, and links from text data\n",
    "df = df.withColumn(\"text\", regexp_replace(col(\"text\"), r'[@#]\\w+|https?://\\S+|\\W', \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e34447de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75fd5cc",
   "metadata": {},
   "source": [
    "## Sentiment Analysis without Tokenization, Lemmatization and Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e939c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sentiment_1 = df.select('date', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd00a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vader SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function for UDF\n",
    "def analyze_sentiment(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']\n",
    "\n",
    "# Save UDF\n",
    "sentiment_udf = udf(analyze_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a26f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis and add the results to a new column\n",
    "df_for_sentiment_1 = df_for_sentiment_1.withColumn(\"sentiment_score\", sentiment_udf(df_for_sentiment_1[\"text\"]))\n",
    "\n",
    "#df_for_sentiment_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54cbd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"date\" column to 'yyyy-MM-dd' format\n",
    "df_for_sentiment_1 = df_for_sentiment_1.withColumn(\"date\", F.to_date(df_for_sentiment_1[\"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd48202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you're using the correct column name in the aggregation\n",
    "daily_sentiment_1 = df_for_sentiment_1.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment_score\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfbbd7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a full date array to include the entire date range\n",
    "min_date = daily_sentiment_1.selectExpr(\"min(date) as min_date\").first().min_date\n",
    "max_date = daily_sentiment_1.selectExpr(\"max(date) as max_date\").first().max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c44a97d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date array\n",
    "date_range = [min_date + timedelta(days=x) for x in range((max_date - min_date).days + 1)]\n",
    "date_range_df = spark.createDataFrame([(date,) for date in date_range], [\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54729282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the gap in date column\n",
    "daily_sentiment_1 = date_range_df.join(daily_sentiment_1, on=[\"date\"], how=\"left\").orderBy(\"date\").fillna(0, subset=[\"avg_sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3522fa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_sentiment_score: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "daily_sentiment_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "246a0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date' contains 0 null values.\n",
      "Column 'avg_sentiment_score' contains 0 null values.\n"
     ]
    }
   ],
   "source": [
    "# Get the column names\n",
    "columns = daily_sentiment_1.columns\n",
    "\n",
    "# Find and print the count of null values in each column\n",
    "for column in columns:\n",
    "    null_count = daily_sentiment_1.filter(daily_sentiment_1[column].isNull()).count()\n",
    "    print(f\"Column '{column}' contains {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f7355",
   "metadata": {},
   "source": [
    "### Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5db85fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a window specification to order the data by date\n",
    "window_spec = Window.orderBy(\"date\")\n",
    "\n",
    "# Calculate the next non-null value using the 'last' function\n",
    "interpolated_df = daily_sentiment_1.withColumn(\"next_value\", F.last(\"avg_sentiment_score\", True).over(window_spec))\n",
    "\n",
    "# Calculate the previous non-null value using the 'first' function\n",
    "interpolated_df = interpolated_df.withColumn(\"prev_value\", F.first(\"avg_sentiment_score\", True).over(window_spec))\n",
    "\n",
    "# Calculate the linearly interpolated value\n",
    "interpolated_df = interpolated_df.withColumn(\n",
    "    \"interpolated_value\",\n",
    "    F.when(F.col(\"avg_sentiment_score\") == 0, (F.col(\"next_value\") + F.col(\"prev_value\")) / 2).otherwise(F.col(\"avg_sentiment_score\"))\n",
    ")\n",
    "\n",
    "# Drop the 'next_value' and 'prev_value' columns if not needed\n",
    "interpolated_df = interpolated_df.drop(\"next_value\", \"prev_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "330457b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- avg_sentiment_score: double (nullable = false)\n",
      " |-- interpolated_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interpolated_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea99aea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 23:37:03,599 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 20:==============================================>         (66 + 1) / 80]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      date| interpolated_value|\n",
      "+----------+-------------------+\n",
      "|2009-04-07| 0.1638104692791461|\n",
      "|2009-04-08|0.08190523463957305|\n",
      "|2009-04-09|0.08190523463957305|\n",
      "|2009-04-10|0.08190523463957305|\n",
      "|2009-04-11|0.08190523463957305|\n",
      "|2009-04-12|0.08190523463957305|\n",
      "|2009-04-13|0.08190523463957305|\n",
      "|2009-04-14|0.08190523463957305|\n",
      "|2009-04-15|0.08190523463957305|\n",
      "|2009-04-16|0.08190523463957305|\n",
      "|2009-04-17|0.08190523463957305|\n",
      "|2009-04-18|0.18738602157202913|\n",
      "|2009-04-19|0.18894100089100915|\n",
      "|2009-04-20|0.17782408521710552|\n",
      "|2009-04-21|0.17327567762269075|\n",
      "|2009-04-22|0.08190523463957305|\n",
      "|2009-04-23|0.08190523463957305|\n",
      "|2009-04-24|0.08190523463957305|\n",
      "|2009-04-25|0.08190523463957305|\n",
      "|2009-04-26|0.08190523463957305|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interpolated_df = interpolated_df.select('date', 'interpolated_value')\n",
    "interpolated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf59918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date' contains 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 23:42:34,536 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 31:=====================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'interpolated_value' contains 0 null values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the column names\n",
    "columns = interpolated_df.columns\n",
    "\n",
    "# Find and print the count of null values in each column\n",
    "for column in columns:\n",
    "    null_count = interpolated_df.filter(interpolated_df[column].isNull()).count()\n",
    "    print(f\"Column '{column}' contains {null_count} null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7688733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 23:46:43,875 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f135a90ebc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start DASH\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Configure DASH\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(\n",
    "        id='line-chart',\n",
    "        figure=px.line(interpolated_df, x='date', y='interpolated_value', title='Daily Average Sentiment Score')\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9d72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f90cf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasgdfhdsh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb7aa70",
   "metadata": {},
   "source": [
    "# Lemmatization, Tokenization, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb97431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "lemmatize_udf = udf(lemmatize_text, StringType())\n",
    "df = df.withColumn(\"text\", lemmatize_udf(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abfb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"filtered_words\")\n",
    "df = tokenizer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0794fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StopWordsRemover on the \"filtered_words\" column in your example DataFrame\n",
    "remover = StopWordsRemover(inputCol=\"filtered_words\", outputCol=\"filtered_words_without_stopwords\")\n",
    "df = remover.transform(df)\n",
    "\n",
    "# You can update the column name as per your needs\n",
    "df = df.withColumnRenamed(\"filtered_words_without_stopwords\", \"filtered_words_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef86ef2",
   "metadata": {},
   "source": [
    "# Preparing the Dataset for the Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just pick the necessary columns\n",
    "df = df.select('0', 'ids', 'date', 'flag', 'user', 'filtered_words_final')\n",
    "\n",
    "# Rename the \"0\" column to \"index\"\n",
    "df = df.withColumnRenamed(\"0\", \"tweet_index\")\n",
    "\n",
    "# Show the result\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of values in the dataframe\n",
    "total_count = df.count()\n",
    "\n",
    "# Show the total count\n",
    "print(\"Total count of values in the dataframe\", total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79986ea3",
   "metadata": {},
   "source": [
    "# MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"password\",\n",
    "    database=\"ProjectTweets\",\n",
    "    charset='utf8mb4',\n",
    "    cursorclass=pymysql.cursors.DictCursor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# # Create a table\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE Tweets (\n",
    "#     tweet_index INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#     ids BIGINT,\n",
    "#     date DATE,\n",
    "#     flag VARCHAR(55),\n",
    "#     user VARCHAR(255),\n",
    "#     filtered_words_final TEXT\n",
    "# );\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ed8b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "#cursor.execute(create_table_sql)\n",
    "\n",
    "# Save changes\n",
    "#connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c36c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aeee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the column named 'filtered_words_final' into a comma-separated column of text.\n",
    "df = df.withColumn('concatenated_words', concat_ws(\",\", df['filtered_words_final']))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select('tweet_index', 'ids', 'date', 'flag', 'user', 'concatenated_words')\n",
    "df.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaf4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql_url = \"jdbc:mysql://localhost:3306/ProjectTweets\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60333aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.jdbc(url=mysql_url, table=\"Tweets\", mode=\"overwrite\", properties=mysql_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Execute the ALTER TABLE query\n",
    "# alter_table_sql = \"ALTER TABLE Tweets ADD COLUMN YCSB_KEY VARCHAR(255);\"\n",
    "# cursor.execute(alter_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc894c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c638b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_mysql = spark.read.jdbc(url=mysql_url, table=\"Tweets\", properties=mysql_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e9d3b",
   "metadata": {},
   "source": [
    "### Showing results from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the database after insertin the dataframe\n",
    "df_from_mysql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1206f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a cursor\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# # Create a table\n",
    "# create_table_sql = \"\"\"\n",
    "# CREATE TABLE YCSB_TEST (\n",
    "#     tweet_index INT AUTO_INCREMENT PRIMARY KEY,\n",
    "#     ids BIGINT,\n",
    "#     date DATE,\n",
    "#     flag VARCHAR(55),\n",
    "#     user VARCHAR(255),\n",
    "#     filtered_words_final TEXT,\n",
    "#     YCSB_KEY VARCHAR(255)\n",
    "# );\n",
    "# \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a table\n",
    "# cursor.execute(create_table_sql)\n",
    "\n",
    "# #Save changes\n",
    "# connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6340c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"/home/hduser/ycsb-0.17.0/bin/ycsb.sh load jdbc -P /home/hduser/ycsb-0.17.0/jdbc-binding/conf/db.properties -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.connection_properties=\\\"user=root&password=password&useSSL=false\\\" -p jdbc.url=jdbc:mysql://localhost:3306/ProjectTweets -p table=YCSB_TEST\"\n",
    "\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"YCSB operation completed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(stdout.decode('utf-8'))\n",
    "else:\n",
    "    print(\"YCSB operation failed. Error message:\")\n",
    "    print(stderr.decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c9c746",
   "metadata": {},
   "source": [
    "### Due to unidentified Issue YCSB did not work. I decided to use cProfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek bir sorgu\n",
    "query = \"SELECT * FROM Tweets WHERE concatenated_words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d222c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cProfile\n",
    "\n",
    "def perform_query():\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    cursor.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cProfile.run(\"perform_query()\", sort=\"cumulative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f15c6d",
   "metadata": {},
   "source": [
    "    Total calls: 1,131,181\n",
    "    Total time: 3.526 seconds\n",
    "\n",
    "Top time-consuming functions:\n",
    "\n",
    "    {built-in method builtins.exec}: 3.526 seconds\n",
    "    <string>:1(<module>): 3.526 seconds\n",
    "    3302925674.py:3(perform_query): 3.524 seconds\n",
    "    cursors.py:133(execute): 3.524 seconds\n",
    "    cursors.py:319(_query): 3.524 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee114bb",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"temp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b560d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ProjectTweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96c9f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ProjectTweets.Tweets (\n",
    "    tweet_index INT,\n",
    "    ids BIGINT,\n",
    "    date DATE,\n",
    "    flag STRING,\n",
    "    user STRING,\n",
    "    concatenated_words STRING\n",
    ")\n",
    "STORED AS PARQUET\n",
    "\"\"\"\n",
    "spark.sql(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_insert_data_sql = \"\"\"\n",
    "INSERT INTO ProjectTweets SELECT * FROM temp_table\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a95903",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(hive_insert_data_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf44c19",
   "metadata": {},
   "source": [
    "### Showing results from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d910743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query\n",
    "result = spark.sql(\"SELECT * FROM ProjectTweets\")\n",
    "\n",
    "# Show Result\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"/home/hduser/ycsb-0.17.0/bin/ycsb.sh load jdbc -P /home/hduser/ycsb-0.17.0/jdbc-binding/conf/db.properties -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.connection_properties=\\\"user=root&password=password\\\" -p jdbc.url=jdbc:hive2://hive_server:10000/ProjectTweets\"\n",
    "\n",
    "process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = process.communicate()\n",
    "\n",
    "if process.returncode == 0:\n",
    "    print(\"YCSB operation completed successfully.\")\n",
    "    print(\"Output:\")\n",
    "    print(stdout.decode('utf-8'))\n",
    "else:\n",
    "    print(\"YCSB operation failed. Error message:\")\n",
    "    print(stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece01872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_hive_script.py\n",
    "def hive_query():\n",
    "    query\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cProfile.run(\"hive_query()\", sort=\"cumulative\")\n",
    "    \n",
    "    # Show the results with pstat\n",
    "    p = pstats.Stats()\n",
    "    p.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327c848c",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS AFTER TOKENIZATION, LEMMATIZATION AND STOPWORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_sentiment_2 = df.select('date', 'concatenated_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8654ddf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Vader SentimentIntensityAnalyzer\n",
    "# sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Define a function for the UDF\n",
    "# def analyze_sentiment(text):\n",
    "#     sentiment = sia.polarity_scores(text)\n",
    "#     return sentiment['compound']\n",
    "\n",
    "# # Save UDF\n",
    "# sentiment_udf = udf(analyze_sentiment, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Vader analysis and add the results to a new column\n",
    "df_for_sentiment_2 = df_for_sentiment_2.withColumn(\"sentiment_score\", sentiment_udf(df_for_sentiment_2[\"concatenated_words\"]))\n",
    "\n",
    "df_for_sentiment_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"date\" column to 'yyyy-MM-dd' format\n",
    "df_for_sentiment_2 = df_for_sentiment_2.withColumn(\"date\", F.to_date(df_for_sentiment_2[\"date\"]))\n",
    "\n",
    "# Create a data frame for date and average sentiment scores\n",
    "daily_sentiment_2 = df_for_sentiment_2.groupBy(\"date\").agg(avg(\"sentiment_score\").alias(\"avg_sentiment_score\")).orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31902890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full date array to include the entire date range\n",
    "min_date = daily_sentiment_2.selectExpr(\"min(date) as min_date\").first().min_date\n",
    "max_date = daily_sentiment_2.selectExpr(\"max(date) as max_date\").first().max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b60af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the date array\n",
    "date_range = [min_date + timedelta(days=x) for x in range((max_date - min_date).days + 1)]\n",
    "date_range_df = spark.createDataFrame([(date,) for date in date_range], [\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing dates\n",
    "daily_sentiment_2 = date_range_df.join(daily_sentiment_2, on=[\"date\"], how=\"left\").orderBy(\"date\").fillna(0, subset=[\"avg_sentiment_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062c55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f15bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Dash application\n",
    "app = dash.Dash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c418ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the layout of the application\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(\n",
    "        id='sentiment-line-chart',\n",
    "        figure=px.line(daily_sentiment_2, x='date', y='avg_sentiment_score', title='Daily Average Sentiment Score')\n",
    "    )\n",
    "])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e6d1a",
   "metadata": {},
   "source": [
    "## This is why I chose the dataframe without lemmatization, tokenization, and stop word removal.\n",
    "- Because there is not enough sentiment score it is almost 0.\n",
    "- In this case removing special symbols and making the text letters smaller helped to keep the meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a844176",
   "metadata": {},
   "source": [
    "# TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules to work with PySpark\n",
    "from pyspark.sql.functions import lag, lead, when, coalesce, expr, col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the original dataframe\n",
    "first_daily_sentiment_1 = daily_sentiment_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03685d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy(\"date\").orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply Backward Fill interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"backward_fill\", lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f04854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for MSE calculation\n",
    "interpolation_1 = daily_sentiment_1.select(\"avg_sentiment_score\", \"backward_fill\")\n",
    "\n",
    "# Calculate the differences and MSE\n",
    "differences_1 = interpolation_1.withColumn(\"difference\", (col(\"avg_sentiment_score\") - col(\"backward_fill\").cast(\"double\")) ** 2)\n",
    "mse_score_1 = differences_1.agg({\"difference\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "print(\"Mean Squared Error (MSE) Score:\", mse_score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e27bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply Quadratic Interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"quadratic_fill\", coalesce(\n",
    "    (lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec) + 2 * daily_sentiment_1[\"avg_sentiment_score\"] - lead(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec)),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387421bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for MSE calculation\n",
    "interpolation_3 = daily_sentiment_1.select(\"avg_sentiment_score\", \"quadratic_fill\")\n",
    "\n",
    "# Calculate the differences and MSE\n",
    "differences_3 = interpolation_3.withColumn(\"difference\", (col(\"avg_sentiment_score\") - col(\"quadratic_fill\").cast(\"double\")) ** 2)\n",
    "mse_score_3 = differences_3.agg({\"difference\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "print(\"Mean Squared Error (MSE) Score:\", mse_score_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply Mean of Nearest Neighbors interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"knn_mean\", coalesce(\n",
    "    (daily_sentiment_1[\"avg_sentiment_score\"] + (lag(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec) + lead(daily_sentiment_1[\"avg_sentiment_score\"]).over(window_spec)) / 2),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for MSE calculation\n",
    "interpolation_4 = daily_sentiment_1.select(\"avg_sentiment_score\", \"knn_mean\")\n",
    "\n",
    "# Calculate the differences and MSE\n",
    "differences_4 = interpolation_4.withColumn(\"difference\", (col(\"avg_sentiment_score\") - col(\"knn_mean\").cast(\"double\")) ** 2)\n",
    "mse_score_4 = differences_4.agg({\"difference\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "print(\"Mean Squared Error (MSE) Score:\", mse_score_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Apply Mean of Seasonal Counterparts interpolation\n",
    "daily_sentiment_1 = daily_sentiment_1.withColumn(\"seasonal_mean\", coalesce(\n",
    "    (daily_sentiment_1[\"avg_sentiment_score\"] + (lag(daily_sentiment_1[\"avg_sentiment_score\"], 7).over(window_spec) + lead(daily_sentiment_1[\"avg_sentiment_score\"], -7).over(window_spec)) / 2),\n",
    "    daily_sentiment_1[\"avg_sentiment_score\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f950d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for MSE calculation\n",
    "interpolation_5 = daily_sentiment_1.select(\"avg_sentiment_score\", \"seasonal_mean\")\n",
    "\n",
    "# Calculate the differences and MSE\n",
    "differences_5 = interpolation_5.withColumn(\"difference\", (col(\"avg_sentiment_score\") - col(\"seasonal_mean\").cast(\"double\")) ** 2)\n",
    "mse_score_5 = differences_5.agg({\"difference\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "print(\"Mean Squared Error (MSE) Score:\", mse_score_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cd308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect the results for visualization\n",
    "interpolated_data = daily_sentiment_1.select(\"date\", \"avg_sentiment_score\", \"backward_fill\", \"quadratic_fill\", \"knn_mean\", \"seasonal_mean\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2a308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "dates = [row.date for row in interpolated_data]\n",
    "original_scores = [row.avg_sentiment_score for row in interpolated_data]\n",
    "interpolation_methods = [\"backward_fill\", \"quadratic_fill\", \"knn_mean\", \"seasonal_mean\"]\n",
    "\n",
    "# Create a loop to plot each interpolation method\n",
    "for method in interpolation_methods:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(dates, original_scores, label='Original')\n",
    "    plt.plot(dates, [row[method] for row in interpolated_data], label=method.replace(\"_\", \" \").title())  # Use the method name as the label\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sentiment Score')\n",
    "    plt.title(f'Original vs. {method.replace(\"_\", \" \").title()}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69228208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have mse_score values in these variables\n",
    "mse_scores = [mse_score_1, mse_score_3, mse_score_4, mse_score_5]\n",
    "\n",
    "# Filter the mse_scores that are between 0.0 and 0.5\n",
    "filtered_mse_scores = [score for score in mse_scores if 0.0 <= score <= 0.5]\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(range(len(filtered_mse_scores)), filtered_mse_scores)\n",
    "plt.xlabel(\"MSE Score Index\")\n",
    "plt.ylabel(\"MSE Score\")\n",
    "plt.title(\"MSE Scores\")\n",
    "plt.xticks(range(len(filtered_mse_scores)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f437289",
   "metadata": {},
   "source": [
    "- Backward fill MSE Score: 0.0096\n",
    "- Quadratic fill MSE Score: 0.0263\n",
    "- KNN Mean MSE Score: 0.0107\n",
    "- Seasonal Mean MSE Score: 0.0140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba1826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff396c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1cbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aec0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38a41e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54c486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687bf2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffd8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609cc47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
